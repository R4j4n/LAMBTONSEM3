{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEAM MEMBERS \n",
    "* * *\n",
    "Rajan Ghimire , STUDENT ID: C0924991\n",
    "\n",
    "* * *\n",
    "\n",
    "Prajwal Luitel , STUDENT ID: C0927658\n",
    "\n",
    "* * *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 4070 Ti SUPER\n",
      "Total Memory: 15.70 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"Total Memory: {torch.cuda.get_device_properties(i).total_memory / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    print(\"No GPU available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aeroserver/miniconda3/envs/cuda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/aeroserver/.cache/kagglehub/datasets/anshtanwar/microscopic-fungi-images/versions/2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"anshtanwar/microscopic-fungi-images\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "\n",
    "from torchvision.models import vgg16\n",
    "vgg_model = vgg16()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.classes = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        \n",
    "        self.samples = []\n",
    "        for class_name in self.classes:\n",
    "            class_dir = self.root_dir / class_name\n",
    "            for img_name in os.listdir(class_dir)[:100]:\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    self.samples.append((\n",
    "                        str(class_dir / img_name),\n",
    "                        self.class_to_idx[class_name]\n",
    "                    ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Apply transforms if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "def create_dataloaders(data_dir, batch_size=32, num_workers=4):\n",
    "    # Define transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = CustomImageDataset(\n",
    "        root_dir=data_dir,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    return dataloader, dataset.classes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader, classes = create_dataloaders(\n",
    "data_dir=f\"{path}/train\",\n",
    "batch_size=batch_size\n",
    ")\n",
    "test_loader, classes = create_dataloaders(\n",
    "data_dir=f\"{path}/test\",\n",
    "batch_size=batch_size\n",
    ")\n",
    "valid_loader, classes = create_dataloaders(\n",
    "data_dir=f\"{path}/valid\",\n",
    "batch_size=batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG Architecture:\n",
      "└─ features: Sequential (14,714,688 parameters)\n",
      "  └─ 0: Conv2d (1,792 parameters)\n",
      "  └─ 1: ReLU (0 parameters)\n",
      "  └─ 2: Conv2d (36,928 parameters)\n",
      "  └─ 3: ReLU (0 parameters)\n",
      "  └─ 4: MaxPool2d (0 parameters)\n",
      "  └─ 5: Conv2d (73,856 parameters)\n",
      "  └─ 6: ReLU (0 parameters)\n",
      "  └─ 7: Conv2d (147,584 parameters)\n",
      "  └─ 8: ReLU (0 parameters)\n",
      "  └─ 9: MaxPool2d (0 parameters)\n",
      "  └─ 10: Conv2d (295,168 parameters)\n",
      "  └─ 11: ReLU (0 parameters)\n",
      "  └─ 12: Conv2d (590,080 parameters)\n",
      "  └─ 13: ReLU (0 parameters)\n",
      "  └─ 14: Conv2d (590,080 parameters)\n",
      "  └─ 15: ReLU (0 parameters)\n",
      "  └─ 16: MaxPool2d (0 parameters)\n",
      "  └─ 17: Conv2d (1,180,160 parameters)\n",
      "  └─ 18: ReLU (0 parameters)\n",
      "  └─ 19: Conv2d (2,359,808 parameters)\n",
      "  └─ 20: ReLU (0 parameters)\n",
      "  └─ 21: Conv2d (2,359,808 parameters)\n",
      "  └─ 22: ReLU (0 parameters)\n",
      "  └─ 23: MaxPool2d (0 parameters)\n",
      "  └─ 24: Conv2d (2,359,808 parameters)\n",
      "  └─ 25: ReLU (0 parameters)\n",
      "  └─ 26: Conv2d (2,359,808 parameters)\n",
      "  └─ 27: ReLU (0 parameters)\n",
      "  └─ 28: Conv2d (2,359,808 parameters)\n",
      "  └─ 29: ReLU (0 parameters)\n",
      "  └─ 30: MaxPool2d (0 parameters)\n",
      "└─ avgpool: AdaptiveAvgPool2d (0 parameters)\n",
      "└─ classifier: Sequential (123,642,856 parameters)\n",
      "  └─ 0: Linear (102,764,544 parameters)\n",
      "  └─ 1: ReLU (0 parameters)\n",
      "  └─ 2: Dropout (0 parameters)\n",
      "  └─ 3: Linear (16,781,312 parameters)\n",
      "  └─ 4: ReLU (0 parameters)\n",
      "  └─ 5: Dropout (0 parameters)\n",
      "  └─ 6: Linear (4,097,000 parameters)\n"
     ]
    }
   ],
   "source": [
    "def print_model_structure(model, indent=0):\n",
    "    for name, child in model.named_children():\n",
    "        params = sum(p.numel() for p in child.parameters())\n",
    "        print(f\"{'  ' * indent}└─ {name}: {child.__class__.__name__} ({params:,} parameters)\")\n",
    "        \n",
    "        # Recursively print child modules if they exist\n",
    "        if list(child.children()):\n",
    "            print_model_structure(child, indent + 1)\n",
    "\n",
    "# Print the model structure\n",
    "print(\"VGG Architecture:\")\n",
    "print_model_structure(vgg_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from sklearn.model_selection import KFold\n",
    "import optuna\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "class CustomVGG16(nn.Module):\n",
    "    def __init__(self, num_classes: int, dropout_rate: float = 0.5):\n",
    "        super(CustomVGG16, self).__init__()\n",
    "        \n",
    "        vgg = vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "        \n",
    "        self.features = vgg.features\n",
    "        \n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TrainingPipeline:\n",
    "    def __init__(self, num_classes: int, device: str = 'cuda'):\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        self.best_params = None\n",
    "        self.best_model = None\n",
    "        \n",
    "    def objective(self, trial: optuna.Trial, train_loader: DataLoader, \n",
    "                 valid_loader: DataLoader) -> float:\n",
    "        lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.7)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "        \n",
    "        model = CustomVGG16(self.num_classes, dropout_rate).to(self.device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.classifier.parameters(), lr=lr)\n",
    "        \n",
    "        epochs = 10  \n",
    "        best_valid_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            valid_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in valid_loader:\n",
    "                    inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    valid_loss += loss.item()\n",
    "                    \n",
    "                    _, predicted = outputs.max(1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            valid_loss = valid_loss / len(valid_loader)\n",
    "            accuracy = correct / total\n",
    "            \n",
    "            # Report intermediate value\n",
    "            trial.report(valid_loss, epoch)\n",
    "            \n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "            \n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "        \n",
    "        return best_valid_loss\n",
    "\n",
    "    def train_final_model(self, train_loader: DataLoader, \n",
    "                         valid_loader: DataLoader, \n",
    "                         test_loader: DataLoader,\n",
    "                         k_folds: int = 5) -> Tuple[nn.Module, Dict]:\n",
    "        kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "        \n",
    "        train_indices = np.arange(len(train_loader.dataset))\n",
    "        \n",
    "        fold_results = []\n",
    "        \n",
    "        # K-fold Cross Validation\n",
    "        for fold, (train_ids, valid_ids) in enumerate(kfold.split(train_indices)):\n",
    "            print(f'FOLD {fold}')\n",
    "            print('--------------------------------')\n",
    "            \n",
    "            train_subsampler = SubsetRandomSampler(train_ids)\n",
    "            valid_subsampler = SubsetRandomSampler(valid_ids)\n",
    "            \n",
    "            train_fold_loader = DataLoader(\n",
    "                train_loader.dataset, \n",
    "                batch_size=self.best_params['batch_size'],\n",
    "                sampler=train_subsampler\n",
    "            )\n",
    "            valid_fold_loader = DataLoader(\n",
    "                train_loader.dataset,\n",
    "                batch_size=self.best_params['batch_size'],\n",
    "                sampler=valid_subsampler\n",
    "            )\n",
    "            \n",
    "            model = CustomVGG16(\n",
    "                self.num_classes,\n",
    "                self.best_params['dropout_rate']\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # Training setup\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.Adam(\n",
    "                model.classifier.parameters(),\n",
    "                lr=self.best_params['lr']\n",
    "            )\n",
    "            \n",
    "            # Training loop\n",
    "            epochs = 30  \n",
    "            best_valid_loss = float('inf')\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                # Training phase\n",
    "                model.train()\n",
    "                train_loss = 0.0\n",
    "                for inputs, labels in train_fold_loader:\n",
    "                    inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    train_loss += loss.item()\n",
    "                \n",
    "                # Validation phase\n",
    "                model.eval()\n",
    "                valid_loss = 0.0\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in valid_fold_loader:\n",
    "                        inputs = inputs.to(self.device)\n",
    "                        labels = labels.to(self.device)\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        valid_loss += loss.item()\n",
    "                        \n",
    "                        _, predicted = outputs.max(1)\n",
    "                        total += labels.size(0)\n",
    "                        correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "                train_loss = train_loss / len(train_fold_loader)\n",
    "                valid_loss = valid_loss / len(valid_fold_loader)\n",
    "                accuracy = correct / total\n",
    "                \n",
    "                print(f'Epoch: {epoch+1}/{epochs}')\n",
    "                print(f'Training Loss: {train_loss:.4f}')\n",
    "                print(f'Validation Loss: {valid_loss:.4f}')\n",
    "                print(f'Validation Accuracy: {accuracy:.4f}')\n",
    "                \n",
    "                if valid_loss < best_valid_loss:\n",
    "                    best_valid_loss = valid_loss\n",
    "                    if fold == 0:  # Save best model from first fold\n",
    "                        self.best_model = model.state_dict()\n",
    "            \n",
    "            fold_results.append(best_valid_loss)\n",
    "        \n",
    "        print('\\nK-FOLD CROSS VALIDATION RESULTS')\n",
    "        print('--------------------------------')\n",
    "        print(f'Average validation loss: {np.mean(fold_results):.4f}')\n",
    "        print(f'Std of validation loss: {np.std(fold_results):.4f}')\n",
    "        \n",
    "        model = CustomVGG16(self.num_classes, self.best_params['dropout_rate']).to(self.device)\n",
    "        model.load_state_dict(self.best_model)\n",
    "        model.eval()\n",
    "        \n",
    "        test_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                \n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        test_loss = test_loss / len(test_loader)\n",
    "        test_accuracy = correct / total\n",
    "        \n",
    "        print('\\nFINAL TEST RESULTS')\n",
    "        print('--------------------------------')\n",
    "        print(f'Test Loss: {test_loss:.4f}')\n",
    "        print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "        \n",
    "        return model, {\n",
    "            'test_loss': test_loss,\n",
    "            'test_accuracy': test_accuracy,\n",
    "            'fold_results': fold_results\n",
    "        }\n",
    "\n",
    "    def find_best_hyperparameters(self, train_loader: DataLoader, \n",
    "                                 valid_loader: DataLoader, \n",
    "                                 n_trials: int = 100) -> Dict:\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(lambda trial: self.objective(trial, train_loader, valid_loader), \n",
    "                      n_trials=n_trials)\n",
    "        \n",
    "        self.best_params = study.best_params\n",
    "        return study.best_params\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-01 13:17:46,622] A new study created in memory with name: no-name-e4abfacb-9508-40e3-9f12-7694d8888d4f\n",
      "[I 2025-02-01 13:18:05,158] Trial 0 finished with value: 1.4390889406204224 and parameters: {'lr': 0.008517362174549684, 'dropout_rate': 0.2672239664535954, 'batch_size': 64}. Best is trial 0 with value: 1.4390889406204224.\n",
      "[I 2025-02-01 13:18:23,527] Trial 1 finished with value: 1.047280205147607 and parameters: {'lr': 0.0006797298690041322, 'dropout_rate': 0.24545322758358784, 'batch_size': 16}. Best is trial 1 with value: 1.047280205147607.\n",
      "[I 2025-02-01 13:18:41,923] Trial 2 finished with value: 1.2471999270575387 and parameters: {'lr': 0.007062023853657059, 'dropout_rate': 0.27383006600049903, 'batch_size': 64}. Best is trial 1 with value: 1.047280205147607.\n",
      "[I 2025-02-01 13:19:00,315] Trial 3 finished with value: 0.9099310551370893 and parameters: {'lr': 0.0006285257834221934, 'dropout_rate': 0.5970128392051451, 'batch_size': 32}. Best is trial 3 with value: 0.9099310551370893.\n",
      "[I 2025-02-01 13:19:18,775] Trial 4 finished with value: 0.8939649547849383 and parameters: {'lr': 0.0001188600851104574, 'dropout_rate': 0.6848642149849223, 'batch_size': 16}. Best is trial 4 with value: 0.8939649547849383.\n",
      "[I 2025-02-01 13:19:37,225] Trial 5 finished with value: 0.8986141894544873 and parameters: {'lr': 9.216813615397013e-05, 'dropout_rate': 0.6155622287165023, 'batch_size': 64}. Best is trial 4 with value: 0.8939649547849383.\n",
      "[I 2025-02-01 13:19:43,473] Trial 6 pruned. \n",
      "[I 2025-02-01 13:19:48,005] Trial 7 pruned. \n",
      "[I 2025-02-01 13:19:52,527] Trial 8 pruned. \n",
      "[I 2025-02-01 13:19:55,301] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.0001188600851104574, 'dropout_rate': 0.6848642149849223, 'batch_size': 16}\n",
      "FOLD 0\n",
      "--------------------------------\n",
      "Epoch: 1/30\n",
      "Training Loss: 1.4902\n",
      "Validation Loss: 1.3070\n",
      "Validation Accuracy: 0.4400\n",
      "Epoch: 2/30\n",
      "Training Loss: 1.1089\n",
      "Validation Loss: 1.1095\n",
      "Validation Accuracy: 0.5800\n",
      "Epoch: 3/30\n",
      "Training Loss: 0.9057\n",
      "Validation Loss: 0.9917\n",
      "Validation Accuracy: 0.6400\n",
      "Epoch: 4/30\n",
      "Training Loss: 0.7559\n",
      "Validation Loss: 0.9663\n",
      "Validation Accuracy: 0.6600\n",
      "Epoch: 5/30\n",
      "Training Loss: 0.5508\n",
      "Validation Loss: 0.9646\n",
      "Validation Accuracy: 0.6300\n",
      "Epoch: 6/30\n",
      "Training Loss: 0.4002\n",
      "Validation Loss: 1.1407\n",
      "Validation Accuracy: 0.6000\n",
      "Epoch: 7/30\n",
      "Training Loss: 0.2739\n",
      "Validation Loss: 1.1142\n",
      "Validation Accuracy: 0.6800\n",
      "Epoch: 8/30\n",
      "Training Loss: 0.2058\n",
      "Validation Loss: 0.9471\n",
      "Validation Accuracy: 0.6400\n",
      "Epoch: 9/30\n",
      "Training Loss: 0.1396\n",
      "Validation Loss: 1.0343\n",
      "Validation Accuracy: 0.6000\n",
      "Epoch: 10/30\n",
      "Training Loss: 0.0946\n",
      "Validation Loss: 1.3907\n",
      "Validation Accuracy: 0.6300\n",
      "Epoch: 11/30\n",
      "Training Loss: 0.0752\n",
      "Validation Loss: 1.0974\n",
      "Validation Accuracy: 0.6500\n",
      "Epoch: 12/30\n",
      "Training Loss: 0.0551\n",
      "Validation Loss: 1.3001\n",
      "Validation Accuracy: 0.6600\n",
      "Epoch: 13/30\n",
      "Training Loss: 0.0379\n",
      "Validation Loss: 1.1703\n",
      "Validation Accuracy: 0.6400\n",
      "Epoch: 14/30\n",
      "Training Loss: 0.0263\n",
      "Validation Loss: 1.1660\n",
      "Validation Accuracy: 0.6500\n",
      "Epoch: 15/30\n",
      "Training Loss: 0.0174\n",
      "Validation Loss: 1.3263\n",
      "Validation Accuracy: 0.6200\n",
      "Epoch: 16/30\n",
      "Training Loss: 0.0176\n",
      "Validation Loss: 1.2490\n",
      "Validation Accuracy: 0.6500\n",
      "Epoch: 17/30\n",
      "Training Loss: 0.0123\n",
      "Validation Loss: 1.3069\n",
      "Validation Accuracy: 0.6500\n",
      "Epoch: 18/30\n",
      "Training Loss: 0.0094\n",
      "Validation Loss: 1.5428\n",
      "Validation Accuracy: 0.6200\n",
      "Epoch: 19/30\n",
      "Training Loss: 0.0107\n",
      "Validation Loss: 1.2294\n",
      "Validation Accuracy: 0.6600\n",
      "Epoch: 20/30\n",
      "Training Loss: 0.0060\n",
      "Validation Loss: 1.4139\n",
      "Validation Accuracy: 0.6900\n",
      "Epoch: 21/30\n",
      "Training Loss: 0.0053\n",
      "Validation Loss: 1.6629\n",
      "Validation Accuracy: 0.6600\n",
      "Epoch: 22/30\n",
      "Training Loss: 0.0046\n",
      "Validation Loss: 1.3044\n",
      "Validation Accuracy: 0.6700\n",
      "Epoch: 23/30\n",
      "Training Loss: 0.0036\n",
      "Validation Loss: 1.5204\n",
      "Validation Accuracy: 0.6700\n",
      "Epoch: 24/30\n",
      "Training Loss: 0.0039\n",
      "Validation Loss: 1.2475\n",
      "Validation Accuracy: 0.6500\n",
      "Epoch: 25/30\n",
      "Training Loss: 0.0033\n",
      "Validation Loss: 1.2963\n",
      "Validation Accuracy: 0.6700\n",
      "Epoch: 26/30\n",
      "Training Loss: 0.0023\n",
      "Validation Loss: 1.3810\n",
      "Validation Accuracy: 0.6500\n",
      "Epoch: 27/30\n",
      "Training Loss: 0.0020\n",
      "Validation Loss: 1.3343\n",
      "Validation Accuracy: 0.6400\n",
      "Epoch: 28/30\n",
      "Training Loss: 0.0026\n",
      "Validation Loss: 1.4840\n",
      "Validation Accuracy: 0.6400\n",
      "Epoch: 29/30\n",
      "Training Loss: 0.0024\n",
      "Validation Loss: 1.4345\n",
      "Validation Accuracy: 0.6400\n",
      "Epoch: 30/30\n",
      "Training Loss: 0.0017\n",
      "Validation Loss: 1.5705\n",
      "Validation Accuracy: 0.6600\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Epoch: 1/30\n",
      "Training Loss: 1.5192\n",
      "Validation Loss: 1.3400\n",
      "Validation Accuracy: 0.4000\n",
      "Epoch: 2/30\n",
      "Training Loss: 1.1423\n",
      "Validation Loss: 1.1347\n",
      "Validation Accuracy: 0.5400\n",
      "Epoch: 3/30\n",
      "Training Loss: 0.9357\n",
      "Validation Loss: 0.9940\n",
      "Validation Accuracy: 0.5500\n",
      "Epoch: 4/30\n",
      "Training Loss: 0.6966\n",
      "Validation Loss: 1.0319\n",
      "Validation Accuracy: 0.5700\n",
      "Epoch: 5/30\n",
      "Training Loss: 0.5985\n",
      "Validation Loss: 1.0288\n",
      "Validation Accuracy: 0.5500\n",
      "Epoch: 6/30\n",
      "Training Loss: 0.4101\n",
      "Validation Loss: 0.9969\n",
      "Validation Accuracy: 0.6100\n",
      "Epoch: 7/30\n",
      "Training Loss: 0.2801\n",
      "Validation Loss: 0.9459\n",
      "Validation Accuracy: 0.6200\n",
      "Epoch: 8/30\n",
      "Training Loss: 0.1982\n",
      "Validation Loss: 1.0721\n",
      "Validation Accuracy: 0.6000\n",
      "Epoch: 9/30\n",
      "Training Loss: 0.1493\n",
      "Validation Loss: 1.0519\n",
      "Validation Accuracy: 0.6200\n",
      "Epoch: 10/30\n",
      "Training Loss: 0.1021\n",
      "Validation Loss: 1.0280\n",
      "Validation Accuracy: 0.6400\n",
      "Epoch: 11/30\n",
      "Training Loss: 0.0630\n",
      "Validation Loss: 1.1585\n",
      "Validation Accuracy: 0.6000\n",
      "Epoch: 12/30\n",
      "Training Loss: 0.0408\n",
      "Validation Loss: 1.1607\n",
      "Validation Accuracy: 0.6300\n",
      "Epoch: 13/30\n",
      "Training Loss: 0.0250\n",
      "Validation Loss: 1.2227\n",
      "Validation Accuracy: 0.6300\n",
      "Epoch: 14/30\n",
      "Training Loss: 0.0287\n",
      "Validation Loss: 1.5016\n",
      "Validation Accuracy: 0.6300\n",
      "Epoch: 15/30\n",
      "Training Loss: 0.0235\n",
      "Validation Loss: 1.3326\n",
      "Validation Accuracy: 0.5900\n",
      "Epoch: 16/30\n",
      "Training Loss: 0.0215\n",
      "Validation Loss: 1.1921\n",
      "Validation Accuracy: 0.6000\n",
      "Epoch: 17/30\n",
      "Training Loss: 0.0157\n",
      "Validation Loss: 1.5708\n",
      "Validation Accuracy: 0.6300\n",
      "Epoch: 18/30\n",
      "Training Loss: 0.0130\n",
      "Validation Loss: 1.2461\n",
      "Validation Accuracy: 0.5900\n",
      "Epoch: 19/30\n",
      "Training Loss: 0.0080\n",
      "Validation Loss: 1.7842\n",
      "Validation Accuracy: 0.5700\n",
      "Epoch: 20/30\n",
      "Training Loss: 0.0093\n",
      "Validation Loss: 1.4383\n",
      "Validation Accuracy: 0.5900\n",
      "Epoch: 21/30\n",
      "Training Loss: 0.0059\n",
      "Validation Loss: 1.6153\n",
      "Validation Accuracy: 0.6100\n",
      "Epoch: 22/30\n",
      "Training Loss: 0.0033\n",
      "Validation Loss: 1.3641\n",
      "Validation Accuracy: 0.6000\n",
      "Epoch: 23/30\n",
      "Training Loss: 0.0051\n",
      "Validation Loss: 1.4988\n",
      "Validation Accuracy: 0.6000\n",
      "Epoch: 24/30\n",
      "Training Loss: 0.0039\n",
      "Validation Loss: 1.4956\n",
      "Validation Accuracy: 0.5900\n",
      "Epoch: 25/30\n",
      "Training Loss: 0.0041\n",
      "Validation Loss: 1.4675\n",
      "Validation Accuracy: 0.5700\n",
      "Epoch: 26/30\n",
      "Training Loss: 0.0039\n",
      "Validation Loss: 1.4150\n",
      "Validation Accuracy: 0.6200\n",
      "Epoch: 27/30\n",
      "Training Loss: 0.0058\n",
      "Validation Loss: 1.4987\n",
      "Validation Accuracy: 0.6200\n",
      "Epoch: 28/30\n",
      "Training Loss: 0.0027\n",
      "Validation Loss: 1.9220\n",
      "Validation Accuracy: 0.6200\n",
      "Epoch: 29/30\n",
      "Training Loss: 0.0027\n",
      "Validation Loss: 1.4987\n",
      "Validation Accuracy: 0.6300\n",
      "Epoch: 30/30\n",
      "Training Loss: 0.0020\n",
      "Validation Loss: 1.5486\n",
      "Validation Accuracy: 0.6200\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Epoch: 1/30\n",
      "Training Loss: 1.5001\n",
      "Validation Loss: 1.2832\n",
      "Validation Accuracy: 0.4800\n",
      "Epoch: 2/30\n",
      "Training Loss: 1.1713\n",
      "Validation Loss: 1.2385\n",
      "Validation Accuracy: 0.5400\n",
      "Epoch: 3/30\n",
      "Training Loss: 0.8752\n",
      "Validation Loss: 0.9862\n",
      "Validation Accuracy: 0.5700\n",
      "Epoch: 4/30\n",
      "Training Loss: 0.6764\n",
      "Validation Loss: 0.9544\n",
      "Validation Accuracy: 0.6600\n",
      "Epoch: 5/30\n",
      "Training Loss: 0.5399\n",
      "Validation Loss: 1.0194\n",
      "Validation Accuracy: 0.6200\n",
      "Epoch: 6/30\n",
      "Training Loss: 0.3685\n",
      "Validation Loss: 1.1081\n",
      "Validation Accuracy: 0.6000\n",
      "Epoch: 7/30\n",
      "Training Loss: 0.2735\n",
      "Validation Loss: 1.0510\n",
      "Validation Accuracy: 0.6400\n",
      "Epoch: 8/30\n",
      "Training Loss: 0.1958\n",
      "Validation Loss: 1.0429\n",
      "Validation Accuracy: 0.6100\n",
      "Epoch: 9/30\n",
      "Training Loss: 0.1257\n",
      "Validation Loss: 1.1952\n",
      "Validation Accuracy: 0.6300\n",
      "Epoch: 10/30\n",
      "Training Loss: 0.0903\n",
      "Validation Loss: 1.0722\n",
      "Validation Accuracy: 0.6300\n",
      "Epoch: 11/30\n",
      "Training Loss: 0.0607\n",
      "Validation Loss: 1.3529\n",
      "Validation Accuracy: 0.6100\n",
      "Epoch: 12/30\n",
      "Training Loss: 0.0413\n",
      "Validation Loss: 1.5251\n",
      "Validation Accuracy: 0.5500\n",
      "Epoch: 13/30\n",
      "Training Loss: 0.0497\n",
      "Validation Loss: 1.4237\n",
      "Validation Accuracy: 0.6200\n",
      "Epoch: 14/30\n",
      "Training Loss: 0.0292\n",
      "Validation Loss: 1.2748\n",
      "Validation Accuracy: 0.6300\n",
      "Epoch: 15/30\n",
      "Training Loss: 0.0239\n",
      "Validation Loss: 1.2072\n",
      "Validation Accuracy: 0.6500\n",
      "Epoch: 16/30\n",
      "Training Loss: 0.0133\n",
      "Validation Loss: 1.4612\n",
      "Validation Accuracy: 0.6400\n",
      "Epoch: 17/30\n",
      "Training Loss: 0.0120\n",
      "Validation Loss: 1.2841\n",
      "Validation Accuracy: 0.6400\n",
      "Epoch: 18/30\n",
      "Training Loss: 0.0125\n",
      "Validation Loss: 1.3628\n",
      "Validation Accuracy: 0.6200\n",
      "Epoch: 19/30\n",
      "Training Loss: 0.0084\n",
      "Validation Loss: 1.6722\n",
      "Validation Accuracy: 0.6700\n",
      "Epoch: 20/30\n",
      "Training Loss: 0.0075\n",
      "Validation Loss: 1.3939\n",
      "Validation Accuracy: 0.6400\n",
      "Epoch: 21/30\n",
      "Training Loss: 0.0056\n",
      "Validation Loss: 1.4102\n",
      "Validation Accuracy: 0.6300\n",
      "Epoch: 22/30\n",
      "Training Loss: 0.0060\n",
      "Validation Loss: 1.6236\n",
      "Validation Accuracy: 0.6300\n",
      "Epoch: 23/30\n",
      "Training Loss: 0.0083\n",
      "Validation Loss: 1.4736\n",
      "Validation Accuracy: 0.6500\n",
      "Epoch: 24/30\n",
      "Training Loss: 0.0046\n",
      "Validation Loss: 1.5193\n",
      "Validation Accuracy: 0.6400\n",
      "Epoch: 25/30\n",
      "Training Loss: 0.0060\n",
      "Validation Loss: 1.6083\n",
      "Validation Accuracy: 0.6300\n",
      "Epoch: 26/30\n",
      "Training Loss: 0.0033\n",
      "Validation Loss: 1.6134\n",
      "Validation Accuracy: 0.6400\n",
      "Epoch: 27/30\n",
      "Training Loss: 0.0034\n",
      "Validation Loss: 1.4387\n",
      "Validation Accuracy: 0.6500\n",
      "Epoch: 28/30\n",
      "Training Loss: 0.0022\n",
      "Validation Loss: 1.5017\n",
      "Validation Accuracy: 0.6500\n",
      "Epoch: 29/30\n",
      "Training Loss: 0.0017\n",
      "Validation Loss: 1.4179\n",
      "Validation Accuracy: 0.6500\n",
      "Epoch: 30/30\n",
      "Training Loss: 0.0022\n",
      "Validation Loss: 1.4022\n",
      "Validation Accuracy: 0.6400\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Epoch: 1/30\n",
      "Training Loss: 1.5068\n",
      "Validation Loss: 1.2823\n",
      "Validation Accuracy: 0.4900\n",
      "Epoch: 2/30\n",
      "Training Loss: 1.1256\n",
      "Validation Loss: 1.0896\n",
      "Validation Accuracy: 0.5600\n",
      "Epoch: 3/30\n",
      "Training Loss: 0.8955\n",
      "Validation Loss: 1.0974\n",
      "Validation Accuracy: 0.6300\n",
      "Epoch: 4/30\n",
      "Training Loss: 0.7285\n",
      "Validation Loss: 1.0682\n",
      "Validation Accuracy: 0.6800\n",
      "Epoch: 5/30\n",
      "Training Loss: 0.5657\n",
      "Validation Loss: 1.1398\n",
      "Validation Accuracy: 0.6200\n",
      "Epoch: 6/30\n",
      "Training Loss: 0.3973\n",
      "Validation Loss: 1.0350\n",
      "Validation Accuracy: 0.6300\n",
      "Epoch: 7/30\n",
      "Training Loss: 0.2912\n",
      "Validation Loss: 0.9920\n",
      "Validation Accuracy: 0.6900\n",
      "Epoch: 8/30\n",
      "Training Loss: 0.2173\n",
      "Validation Loss: 0.9240\n",
      "Validation Accuracy: 0.6800\n",
      "Epoch: 9/30\n",
      "Training Loss: 0.1875\n",
      "Validation Loss: 1.1612\n",
      "Validation Accuracy: 0.6500\n",
      "Epoch: 10/30\n",
      "Training Loss: 0.1071\n",
      "Validation Loss: 1.0484\n",
      "Validation Accuracy: 0.6400\n",
      "Epoch: 11/30\n",
      "Training Loss: 0.0774\n",
      "Validation Loss: 1.0435\n",
      "Validation Accuracy: 0.7000\n",
      "Epoch: 12/30\n",
      "Training Loss: 0.0417\n",
      "Validation Loss: 0.9723\n",
      "Validation Accuracy: 0.6800\n",
      "Epoch: 13/30\n",
      "Training Loss: 0.0295\n",
      "Validation Loss: 0.9469\n",
      "Validation Accuracy: 0.6700\n",
      "Epoch: 14/30\n",
      "Training Loss: 0.0233\n",
      "Validation Loss: 0.9889\n",
      "Validation Accuracy: 0.6900\n",
      "Epoch: 15/30\n",
      "Training Loss: 0.0285\n",
      "Validation Loss: 1.0132\n",
      "Validation Accuracy: 0.6800\n",
      "Epoch: 16/30\n",
      "Training Loss: 0.0207\n",
      "Validation Loss: 1.4096\n",
      "Validation Accuracy: 0.6500\n",
      "Epoch: 17/30\n",
      "Training Loss: 0.0153\n",
      "Validation Loss: 1.2209\n",
      "Validation Accuracy: 0.6700\n",
      "Epoch: 18/30\n",
      "Training Loss: 0.0106\n",
      "Validation Loss: 1.1648\n",
      "Validation Accuracy: 0.6900\n",
      "Epoch: 19/30\n",
      "Training Loss: 0.0148\n",
      "Validation Loss: 1.0745\n",
      "Validation Accuracy: 0.6600\n",
      "Epoch: 20/30\n",
      "Training Loss: 0.0132\n",
      "Validation Loss: 1.1343\n",
      "Validation Accuracy: 0.6800\n",
      "Epoch: 21/30\n",
      "Training Loss: 0.0081\n",
      "Validation Loss: 1.1441\n",
      "Validation Accuracy: 0.7000\n",
      "Epoch: 22/30\n",
      "Training Loss: 0.0049\n",
      "Validation Loss: 1.1087\n",
      "Validation Accuracy: 0.7100\n",
      "Epoch: 23/30\n",
      "Training Loss: 0.0045\n",
      "Validation Loss: 1.2174\n",
      "Validation Accuracy: 0.6800\n",
      "Epoch: 24/30\n",
      "Training Loss: 0.0047\n",
      "Validation Loss: 1.2191\n",
      "Validation Accuracy: 0.6800\n",
      "Epoch: 25/30\n",
      "Training Loss: 0.0035\n",
      "Validation Loss: 1.3224\n",
      "Validation Accuracy: 0.6500\n",
      "Epoch: 26/30\n",
      "Training Loss: 0.0049\n",
      "Validation Loss: 1.1811\n",
      "Validation Accuracy: 0.7000\n",
      "Epoch: 27/30\n",
      "Training Loss: 0.0030\n",
      "Validation Loss: 1.3826\n",
      "Validation Accuracy: 0.6700\n",
      "Epoch: 28/30\n",
      "Training Loss: 0.0027\n",
      "Validation Loss: 1.3057\n",
      "Validation Accuracy: 0.6700\n",
      "Epoch: 29/30\n",
      "Training Loss: 0.0018\n",
      "Validation Loss: 1.2281\n",
      "Validation Accuracy: 0.6800\n",
      "Epoch: 30/30\n",
      "Training Loss: 0.0018\n",
      "Validation Loss: 1.2144\n",
      "Validation Accuracy: 0.6700\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Epoch: 1/30\n",
      "Training Loss: 1.4903\n",
      "Validation Loss: 1.2414\n",
      "Validation Accuracy: 0.5200\n",
      "Epoch: 2/30\n",
      "Training Loss: 1.1068\n",
      "Validation Loss: 1.1117\n",
      "Validation Accuracy: 0.5400\n",
      "Epoch: 3/30\n",
      "Training Loss: 0.8731\n",
      "Validation Loss: 0.9979\n",
      "Validation Accuracy: 0.5700\n",
      "Epoch: 4/30\n",
      "Training Loss: 0.6802\n",
      "Validation Loss: 0.9926\n",
      "Validation Accuracy: 0.5700\n",
      "Epoch: 5/30\n",
      "Training Loss: 0.4975\n",
      "Validation Loss: 0.9742\n",
      "Validation Accuracy: 0.5800\n",
      "Epoch: 6/30\n",
      "Training Loss: 0.3741\n",
      "Validation Loss: 1.0470\n",
      "Validation Accuracy: 0.6300\n",
      "Epoch: 7/30\n",
      "Training Loss: 0.2838\n",
      "Validation Loss: 1.0059\n",
      "Validation Accuracy: 0.6500\n",
      "Epoch: 8/30\n",
      "Training Loss: 0.1682\n",
      "Validation Loss: 1.0923\n",
      "Validation Accuracy: 0.6300\n",
      "Epoch: 9/30\n",
      "Training Loss: 0.1291\n",
      "Validation Loss: 1.1440\n",
      "Validation Accuracy: 0.5900\n",
      "Epoch: 10/30\n",
      "Training Loss: 0.0873\n",
      "Validation Loss: 1.2672\n",
      "Validation Accuracy: 0.5800\n",
      "Epoch: 11/30\n",
      "Training Loss: 0.0478\n",
      "Validation Loss: 1.4527\n",
      "Validation Accuracy: 0.5900\n",
      "Epoch: 12/30\n",
      "Training Loss: 0.0447\n",
      "Validation Loss: 1.3866\n",
      "Validation Accuracy: 0.6200\n",
      "Epoch: 13/30\n",
      "Training Loss: 0.0250\n",
      "Validation Loss: 1.2994\n",
      "Validation Accuracy: 0.6000\n",
      "Epoch: 14/30\n",
      "Training Loss: 0.0210\n",
      "Validation Loss: 1.5874\n",
      "Validation Accuracy: 0.5700\n",
      "Epoch: 15/30\n",
      "Training Loss: 0.0155\n",
      "Validation Loss: 1.4503\n",
      "Validation Accuracy: 0.6100\n",
      "Epoch: 16/30\n",
      "Training Loss: 0.0154\n",
      "Validation Loss: 1.5662\n",
      "Validation Accuracy: 0.5800\n",
      "Epoch: 17/30\n",
      "Training Loss: 0.0092\n",
      "Validation Loss: 1.6310\n",
      "Validation Accuracy: 0.5700\n",
      "Epoch: 18/30\n",
      "Training Loss: 0.0092\n",
      "Validation Loss: 1.5060\n",
      "Validation Accuracy: 0.6100\n",
      "Epoch: 19/30\n",
      "Training Loss: 0.0094\n",
      "Validation Loss: 1.5214\n",
      "Validation Accuracy: 0.5800\n",
      "Epoch: 20/30\n",
      "Training Loss: 0.0055\n",
      "Validation Loss: 1.8215\n",
      "Validation Accuracy: 0.5800\n",
      "Epoch: 21/30\n",
      "Training Loss: 0.0076\n",
      "Validation Loss: 1.6979\n",
      "Validation Accuracy: 0.5800\n",
      "Epoch: 22/30\n",
      "Training Loss: 0.0044\n",
      "Validation Loss: 1.6070\n",
      "Validation Accuracy: 0.5800\n",
      "Epoch: 23/30\n",
      "Training Loss: 0.0046\n",
      "Validation Loss: 1.6807\n",
      "Validation Accuracy: 0.5900\n",
      "Epoch: 24/30\n",
      "Training Loss: 0.0044\n",
      "Validation Loss: 1.6130\n",
      "Validation Accuracy: 0.5900\n",
      "Epoch: 25/30\n",
      "Training Loss: 0.0035\n",
      "Validation Loss: 1.5739\n",
      "Validation Accuracy: 0.5800\n",
      "Epoch: 26/30\n",
      "Training Loss: 0.0034\n",
      "Validation Loss: 1.8657\n",
      "Validation Accuracy: 0.5800\n",
      "Epoch: 27/30\n",
      "Training Loss: 0.0027\n",
      "Validation Loss: 1.7425\n",
      "Validation Accuracy: 0.5900\n",
      "Epoch: 28/30\n",
      "Training Loss: 0.0018\n",
      "Validation Loss: 1.7484\n",
      "Validation Accuracy: 0.5800\n",
      "Epoch: 29/30\n",
      "Training Loss: 0.0016\n",
      "Validation Loss: 1.8825\n",
      "Validation Accuracy: 0.5900\n",
      "Epoch: 30/30\n",
      "Training Loss: 0.0015\n",
      "Validation Loss: 1.5828\n",
      "Validation Accuracy: 0.6000\n",
      "\n",
      "K-FOLD CROSS VALIDATION RESULTS\n",
      "--------------------------------\n",
      "Average validation loss: 0.9491\n",
      "Std of validation loss: 0.0161\n",
      "\n",
      "FINAL TEST RESULTS\n",
      "--------------------------------\n",
      "Test Loss: 1.4697\n",
      "Test Accuracy: 0.5556\n",
      "Final results: {'test_loss': 1.4697187415191106, 'test_accuracy': 0.5555555555555556, 'fold_results': [0.9471008011272976, 0.9459211187703269, 0.9543661049434117, 0.9240345869745527, 0.974217700106757]}\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    num_classes = len(classes)  \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    pipeline = TrainingPipeline(num_classes=num_classes, device=device)\n",
    "    \n",
    "    best_params = pipeline.find_best_hyperparameters(\n",
    "        train_loader=train_loader,\n",
    "        valid_loader=valid_loader,\n",
    "        n_trials=10\n",
    "    )\n",
    "    print(\"Best hyperparameters:\", best_params)\n",
    "    \n",
    "    model, results = pipeline.train_final_model(\n",
    "        train_loader=train_loader,\n",
    "        valid_loader=valid_loader,\n",
    "        test_loader=test_loader,\n",
    "        k_folds=5\n",
    "    )\n",
    "    \n",
    "    print(\"Final results:\", results)\n",
    "    \n",
    "    torch.save(model.state_dict(), 'best_model.pth')\n",
    "    \n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameter tuning results (using Optuna) show:\n",
    "\n",
    "10 trials were attempted (0-9)\n",
    "The best performing model was found in trial 4 with these parameters:\n",
    "\n",
    "Learning rate: 0.0001188 (a relatively small learning rate)\n",
    "Dropout rate: 0.6848 (fairly aggressive dropout to prevent overfitting)\n",
    "Batch size: 16 (smaller batch size)\n",
    "\n",
    "\n",
    "Trials 6-9 were \"pruned\" (stopped early) because they weren't performing well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
