{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "\n",
    "config = PeftConfig.from_pretrained(\"Rajan/training_run\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NumbersStation/nsql-350M\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"NumbersStation/nsql-350M\")\n",
    "model = PeftModel.from_pretrained(base_model, \"Rajan/training_run\")\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since NumbersStation/NSText2SQL couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/zeus/.cache/huggingface/datasets/NumbersStation___ns_text2_sql/default/0.0.0/e77eeb9e172a1734493ee5bc63b883e9e831c6cf (last modified on Sat Jul 27 16:06:39 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows removed: 232083\n",
      "Percentage of rows removed: 80.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12988/2163250706.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['instruction'] = filtered_df['instruction'].apply(replace_database)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# load the dataset\n",
    "dataset = load_dataset(\"NumbersStation/NSText2SQL\",split=\"train\")\n",
    "\n",
    "\n",
    "# convert the dataset into a pandas dataframe\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "def count_tables(instruction: str)-> int:\n",
    "    \"\"\" Count the number of CREATE TABLE statements in the instruction \"\"\"\n",
    "    return len(re.findall(r'CREATE TABLE', instruction))\n",
    "\n",
    "\n",
    "def replace_database(instruction:str)->str:\n",
    "    \"\"\" Replace the valid SQLite database with a random database \"\"\"\n",
    "    databases = ['SQLite', 'MySQL', 'PostgreSQL']\n",
    "    chosen_db = random.choice(databases)\n",
    "    return re.sub(r'Using valid SQLite', f'Using valid {chosen_db}', instruction)\n",
    "\n",
    "# filter the dataframe so that we have at least 3 tables and at most 10 tables.\n",
    "filtered_df = df[(df['instruction'].apply(count_tables) > 3) & (df['instruction'].apply(count_tables) < 10)]\n",
    "\n",
    "# add MySQL and PostgreSQL to the prompt.\n",
    "filtered_df['instruction'] = filtered_df['instruction'].apply(replace_database)\n",
    "\n",
    "\n",
    "# Calculate the number of rows removed\n",
    "rows_removed = df.shape[0] - filtered_df.shape[0]\n",
    "\n",
    "# Calculate the percentage of rows removed\n",
    "percentage_removed = (rows_removed / df.shape[0]) * 100\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of rows removed: {rows_removed}\")\n",
    "print(f\"Percentage of rows removed: {percentage_removed:.2f}%\")\n",
    "\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "filtered_dataset = Dataset.from_pandas(filtered_df)\n",
    "\n",
    "filtered_dataset = filtered_dataset.remove_columns(\"__index_level_0__\")\n",
    "\n",
    "\n",
    "# do the train test split \n",
    "filtered_dataset = filtered_dataset.train_test_split(test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datset = filtered_dataset[\"test\"]\n",
    "dataset = test_datset.select(range(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78549fbe2f594bdaae6945a3d87f921e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import  tqdm\n",
    "\n",
    "def extract_and_correct_sql(text, correct=False):\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    start_index = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip().upper().startswith(\"SELECT\"):\n",
    "            start_index = i\n",
    "            break\n",
    "\n",
    "    generated_sql = \"\\n\".join(lines[start_index:])\n",
    "\n",
    "    if correct:\n",
    "        if not generated_sql.strip().endswith(\";\"):\n",
    "            generated_sql = generated_sql.strip() + \";\"\n",
    "\n",
    "    return generated_sql\n",
    "\n",
    "# Function to generate SQL and extract SELECT command\n",
    "def generate_and_extract(instruction):\n",
    "    text = f\"{instruction}\\nSELECT\"\n",
    "    model_input = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    generated_ids = model.generate(**model_input, max_new_tokens=100)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    select_commands = extract_and_correct_sql(generated_text,correct=True)\n",
    "    \n",
    "    return select_commands if select_commands else \"\"\n",
    "\n",
    "# Generate predictions and create a DataFrame\n",
    "data = []\n",
    "for row in tqdm(dataset):\n",
    "    instruction = row[\"instruction\"]\n",
    "    output = row[\"output\"]\n",
    "    predicted = generate_and_extract(instruction)\n",
    "    data.append({\"instruction\": instruction, \"output\": output, \"predicted\": predicted})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"final_dataframe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9174434087882823\n",
      "Accuracy: 0.7162162162162162\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, accuracy_score\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "# Initialize counts\n",
    "total_tp = 0\n",
    "total_fp = 0\n",
    "total_fn = 0\n",
    "\n",
    "for i in range(len(df)):\n",
    "    output_tokens = set(tokenize(df.loc[i, 'output']))\n",
    "    predicted_tokens = set(tokenize(df.loc[i, 'predicted']))\n",
    "    \n",
    "    tp = len(output_tokens & predicted_tokens)\n",
    "    fp = len(predicted_tokens - output_tokens)\n",
    "    fn = len(output_tokens - predicted_tokens)\n",
    "    \n",
    "    total_tp += tp\n",
    "    total_fp += fp\n",
    "    total_fn += fn\n",
    "\n",
    "# Calculate Precision and Accuracy\n",
    "precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "accuracy = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "\n",
    "\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9174434087882823\n",
      "Recall: 0.7162162162162162\n",
      "F1 Score: 0.804436660828955\n",
      "Accuracy: 0.7162162162162162\n"
     ]
    }
   ],
   "source": [
    "# Initialize counts\n",
    "total_tp = 0\n",
    "total_fp = 0\n",
    "total_fn = 0\n",
    "total_tokens = 0\n",
    "\n",
    "for i in range(len(df)):\n",
    "    output_tokens = set(tokenize(df.loc[i, 'output']))\n",
    "    predicted_tokens = set(tokenize(df.loc[i, 'predicted']))\n",
    "    \n",
    "    tp = len(output_tokens & predicted_tokens)\n",
    "    fp = len(predicted_tokens - output_tokens)\n",
    "    fn = len(output_tokens - predicted_tokens)\n",
    "    \n",
    "    total_tp += tp\n",
    "    total_fp += fp\n",
    "    total_fn += fn\n",
    "    total_tokens += len(output_tokens)\n",
    "\n",
    "# Calculate Precision, Recall, F1 Score, and Accuracy\n",
    "precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "accuracy = total_tp / total_tokens if total_tokens > 0 else 0\n",
    "\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1_score}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
